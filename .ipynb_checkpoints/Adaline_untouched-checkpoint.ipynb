{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''Load this code, you don't have to change anything'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "def predict(X, w):\n",
    "    a = X.dot(w) < 0  \n",
    "    a = a.astype(int)  # will be True if less than zero\n",
    "    a[a == 1] = -1     # hence gets the label -1\n",
    "    a[a == 0] = 1      # else gets the label 1 b/c greater than 0\n",
    "    return a\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    return np.sum(predict(X, w) == y)/y.size\n",
    "\n",
    "def loss(weights, features, labels):\n",
    "    return np.sum((labels - features.dot(weights)) ** 2)\n",
    "\n",
    "def grad(weights, features, labels):\n",
    "    a =  -2 * np.multiply(features, (labels - features.dot(weights)))\n",
    "    return np.sum(a, axis=0).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "N = 100\n",
    "X = iris.data[:, :2][0:N]  # we only take the first two features and 100 rows\n",
    "y = iris.target[0:N]       # we only take the first two features and 100 rows\n",
    " \n",
    "N, F = X.shape\n",
    "\n",
    "y = y.reshape(N,1)\n",
    "\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Initialize the weights\n",
    "\n",
    "w = np.random.rand(F).reshape(F,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building intuition for the loss function \n",
    "\n",
    "Describe the loss function for Adeline in your own words. What does the equation say? \n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the existing code\n",
    "\n",
    "Using the slides on Adeline as a guide, please examine the `predict`, `accuracy`, `loss` and `grad` functions above. Please describe in your own words what each function is doing \n",
    "\n",
    "[Type your answer here, describing each function]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: Random search\n",
    "\n",
    "In a minute we will optimize Adeline via a gradient-based technique. But first, let's take a minute to understand *why* we even want to do this and *what* we are even doing in the first place. We can also optimize the method via random search. To be clear, this is a bad way to actually optimize the function. But it is a great way to build intution for what is actually happening in gradient descent.\n",
    "\n",
    "Start off by implementing the `get_random_weights` function below. The function should return weights returned at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_weights(NumFeatures=2):\n",
    "    '''\n",
    "    Return a vector of shape NumFeatures X 1, filled at random. Numpy has functions for this\n",
    "    \n",
    "    For this assignment NumFeatures=2\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75496556],\n",
       "       [0.78447617]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_search(features, labels, iters=100):\n",
    "    '''\n",
    "    Implement this random search function\n",
    "    Pseudocode is provided for you. Random search\n",
    "    just keeps trying different weights at random\n",
    "    and then returns the best weights it has found so far\n",
    "    '''\n",
    "    \n",
    "    # initialize current_w and best_loss\n",
    "    current_w = get_random_weights(2)\n",
    "    best_loss = 10000\n",
    "    \n",
    "    for i in range(iters):\n",
    "        new_w = get_random_weights(2)\n",
    "        new_loss = loss(new_w, features, labels)\n",
    "        \n",
    "        # Implement the following... \n",
    "        # If the new_loss is less than the best_loss \n",
    "        # Then replace current_w with new_w and replace best_loss with the new loss\n",
    "        \n",
    "    # return the best weights you found, during your random search\n",
    "    return current_w\n",
    "\n",
    "\n",
    "random_search(features=X, labels=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Optimization: Gradient descent\n",
    "\n",
    "Random search is not a great way to optimize most functions, especially if you know the gradient which always points in the direction of greatest increase of a function. We will optimize Adeline with gradient *descent*, which takes steps in the *opposite* direction of the gradient. Why does that make sense? Hint: you should describe the Adeline loss function?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n",
      "24.917729115175234\n"
     ]
    }
   ],
   "source": [
    "ETA = .0001\n",
    "ITERS = 100\n",
    "\n",
    "# Using the slide on Gradient descent as a guide (along with the pseudocode below)\n",
    "# finish the implementation of gradient descent below\n",
    "\n",
    "for i in range(ITERS):\n",
    "\n",
    "    old_loss = loss(weights=w, features=X, labels=y)\n",
    "\n",
    "    ## implement the gradient descent step here\n",
    "    \n",
    "    new_loss = loss(weights=w, features=X, labels=y)\n",
    "\n",
    "    ## the new_loss should be smaller than the old_loss, if you used eta = .0001\n",
    "    \n",
    "    print(loss(weights=w, features=X, labels=y))  # this should go down each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss by iteration\n",
    "\n",
    "Modify the gradient descent code to plot the loss at each iteration of the algorithm. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search vs. gradient descent\n",
    "\n",
    "Examine the value of the loss after 100 iterations of random search and 100 iterations of gradient descent. Which one optimizes faster? Does that make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by loss \n",
    "\n",
    "Modify the gradient descent code to use the `accuracy` function to measure the accuracy at each iteration of the algorithm. Plot the relationship between accuracy and loss. What do you observe? Does that make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates\n",
    "\n",
    "Try varing the learning rate eta by increasing or decreasing eta by powers of 10. Make a plot showing the learning rate and the accuracy after 100 iterations, for different valuse of eta. Does the algorithm achieve high accuracy for all eta, or only for some learning rates? Why might this be the case?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
