{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "all_results = []\n",
    "\n",
    "with open(\"class.csv\", \"r\") as inf:\n",
    "    results = csv.reader(inf)\n",
    "    next(results)\n",
    "    for r in results:\n",
    "        annotator, annotation, review = r\n",
    "        all_results.append({\"annotator\": annotator, \"annotation\": annotation, \"review\": review})\n",
    "        \n",
    "    \n",
    "\n",
    "## how many data points are there?\n",
    "print(len(all_results))\n",
    "\n",
    "## how many annotators are there?\n",
    "annotators = set()\n",
    "from collections import defaultdict\n",
    "review2judgements = defaultdict(list)\n",
    "\n",
    "for result in all_results:\n",
    "    review2judgements[result[\"review\"]].append({\"annotator\": result[\"annotator\"], \"annotation\": result[\"annotation\"]})\n",
    "    annotators.add(result[\"annotator\"])\n",
    "    \n",
    "len(annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0022988505747126436"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pairwise_agreement(results):\n",
    "    '''\n",
    "    Compute the pairwise agreement between raters for the input results\n",
    "    \n",
    "    To compute pairwise agreement compare judgements from all pairs of annotators for a given item\n",
    "    Return the fraction of pairs of annotators who agree\n",
    "    '''\n",
    "    total_judgements = {}\n",
    "    for result in results:\n",
    "        for other_result in results:\n",
    "            if result[\"annotator\"] != other_result[\"annotator\"]:\n",
    "                pair = [result['annotator'], other_result[\"annotator\"]]\n",
    "                pair.sort()\n",
    "                pair = \"-\".join(pair)\n",
    "                total_judgements[pair] = (result[\"annotation\"], other_result[\"annotation\"])\n",
    "                \n",
    "    out = total_judgements.values()\n",
    "    agrees = 0\n",
    "    for pair in out:\n",
    "        judgement1, judgement2 = pair\n",
    "        if judgement1 == judgement2:\n",
    "            agrees += 1\n",
    "        return agrees/len(out)\n",
    "\n",
    "review4 = {\"1\": 1, \"2\": 0, \"3\": 1}\n",
    "\n",
    "# do 1 and 2 agree == No\n",
    "\n",
    "# do 1 and 3 agree == Yes\n",
    "\n",
    "# do 2 and 3 agree == No\n",
    "\n",
    "# agreement rate = number of agreements / number of pairs: 1/3 \n",
    "\n",
    "pairwise_agreement(review2judgements[review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I visited the Old Town Tortilla factory about five years ago with fond memories so when back in Scottsdale tonight I decided to give it another go. I should have read the Yelp reviews before going, my experience was nothing special. My waiter was friendly enough but the food was just OK. I ordered the Grilled Mahi Mahi Fish Tacos. Upon my waiters advice I order the \\\\\"sauce\\\\\" on the side because I was concerned about them being too spicy. What I received was three \\\\\"chunks of fish\\\\\" on three mini tortillas with four small condiment bowls containing the black beans, jalape\\\\u00f1o sauce, guacamole and a white cucumber sauce? It just looked kinda strange and the fish wasn\\'t all that fresh. I didn\\'t complain because my waiter only asked if I wanted dessert.  Maybe it\\'s me but it just wasn\\'t what I expected. I think there are a lot of other good choices in Scottsdale.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_ = 1\n",
    "review = \"\"\n",
    "for review in review2judgements:\n",
    "    agreement_rate = pairwise_agreement(review2judgements[review])\n",
    "    if agreement_rate < min_:\n",
    "        min_ = agreement_rate\n",
    "        min_review = review\n",
    "min_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-item analysis\n",
    "\n",
    "- Which review has the highest and lowest pairwise agreement rate? Does this make sense?\n",
    "\n",
    "Unable to finish in classtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agreement rate\n",
    "\n",
    "If two reviewers answered randomly (meaning just picked random annotations) how often would they agree just by chance?\n",
    "\n",
    "0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fleiss Kappa\n",
    "\n",
    "[Fleiss kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa) measures the exent to which pairs of reviewers agree, as compared to how much they would agree by chance. \n",
    "\n",
    "- $\\bar{P}_e$ is the rate at which reviewers agree by chance \n",
    "- $\\bar{P}$ is the pairwise agreement rate across all items the dataset\n",
    "    - note: the Wikipedia article uses a slightly different definition of $\\bar{P}$, because it assumes all reviewers review all items, which is not true in our case\n",
    "\n",
    "\n",
    "$\\kappa = \\frac{\\bar{P} - \\bar{P}_e}{1-\\bar{P}_e}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the highest possible value of Fleiss Kappa? What is the lowest?\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does the denominator mean? If $\\bar{P}_e$ is high, then is the denominator high or low?\n",
    "The denominator is the probability that the reviewers don't agree by chance. If $\\bar{P}_e$ is high, the denominator is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is high, do you think the task is well-defined?\n",
    "\n",
    "Inconclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is low, do you think the task is well-defined?\n",
    "\n",
    "Yes, because the reviewers agree at a high rate even though the probability that they agreeby chance is low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you think the Fleiss Kappa will be for the Yelp data set? Do you think it will be higher or lower than for the emotions dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Fleiss Kappa for the dataset\n",
    "\n",
    "def kappa(Pe, Pbar):\n",
    "    \n",
    "    return (Pbar - Pe)/(1 - Pe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
