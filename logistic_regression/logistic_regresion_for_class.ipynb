{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CU Boulder 4604/5604\n",
    "\n",
    "#### September 28, 2020\n",
    "\n",
    "####  Logistic regression revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression revisited\n",
    "\n",
    "- Last week we learned about logistic regression and regularization\n",
    "- Today, we are going modify the notebook from last week in a few ways \n",
    "\n",
    "##### New stuff:\n",
    "1. I filled in the missing functions from last week if you are curious. Everyone who submitted last week got full credit. These notebooks are a way to get your hands dirty with ML code so you learn the materials more deeply. They are not a way for me to quiz you on what you know. That is what HW, quizzes, exams are for.\n",
    "2. We are going to add a regularization term to the loss function\n",
    "3. We will introduce stochastic gradient descent. The well-known ML researcher [Dave Blei](http://www.cs.columbia.edu/~blei/) says stoachstic gradient descent is like walking from New York to Los Angeles by asking one person at a time for directions. And every person you ask for directions is drunk. Even though this seems like a bad way to get across the country, we will see that is has clear advantages\n",
    "4. I changed the code from gradient ascent to gradient descent (flipping signs as needed). Recall that maximizing log likelihood is the same as minimizing negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annoucements \n",
    "\n",
    "- HW2 is out\n",
    "- This notebook is pretty similar to HW2\n",
    "- In the real world, it is OK to lean on sklearn's implementation of logistic regression\n",
    "- But taking this class gives you a chance to actually understand how it works. This notebook should make the sklearn code much less mysterious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 5793.811507862259, accuracy: 0.8196\n",
      "iter: 1, loss: 5790.394373029358, accuracy: 0.8196\n",
      "iter: 2, loss: 5786.852617700935, accuracy: 0.8196\n",
      "iter: 3, loss: 5782.6129072522435, accuracy: 0.8196\n",
      "iter: 4, loss: 5780.136026044902, accuracy: 0.8196\n",
      "iter: 5, loss: 5777.921271115245, accuracy: 0.8196\n",
      "iter: 6, loss: 5775.960619615353, accuracy: 0.8196\n",
      "iter: 7, loss: 5773.633381017308, accuracy: 0.8196\n",
      "iter: 8, loss: 5771.149291397594, accuracy: 0.8196\n",
      "iter: 9, loss: 5767.698772255626, accuracy: 0.8196\n",
      "iter: 10, loss: 5764.169855371659, accuracy: 0.8196\n",
      "iter: 11, loss: 5762.288864998929, accuracy: 0.8196\n",
      "iter: 12, loss: 5756.713485871317, accuracy: 0.8196\n",
      "iter: 13, loss: 5753.129485598977, accuracy: 0.8196\n",
      "iter: 14, loss: 5749.279896535418, accuracy: 0.8196\n",
      "iter: 15, loss: 5746.273395537522, accuracy: 0.8196\n",
      "iter: 16, loss: 5740.713012129488, accuracy: 0.8196\n",
      "iter: 17, loss: 5735.8874671459, accuracy: 0.8196\n",
      "iter: 18, loss: 5733.342031766168, accuracy: 0.8196\n",
      "iter: 19, loss: 5730.165051177558, accuracy: 0.8196\n",
      "iter: 20, loss: 5727.730630038089, accuracy: 0.8196\n",
      "iter: 21, loss: 5725.399834033234, accuracy: 0.8196\n",
      "iter: 22, loss: 5721.554980711097, accuracy: 0.8196\n",
      "iter: 23, loss: 5717.798891945247, accuracy: 0.8196\n",
      "iter: 24, loss: 5716.799460325401, accuracy: 0.8196\n",
      "iter: 25, loss: 5714.708187517938, accuracy: 0.8196\n",
      "iter: 26, loss: 5712.537340745722, accuracy: 0.8196\n",
      "iter: 27, loss: 5710.806096568322, accuracy: 0.8196\n",
      "iter: 28, loss: 5707.868164899273, accuracy: 0.8196\n",
      "iter: 29, loss: 5704.650561276302, accuracy: 0.8196\n",
      "iter: 30, loss: 5700.49810439126, accuracy: 0.8196\n",
      "iter: 31, loss: 5697.6390632079265, accuracy: 0.8196\n",
      "iter: 32, loss: 5694.680205458733, accuracy: 0.8196\n",
      "iter: 33, loss: 5692.031356700263, accuracy: 0.8196\n",
      "iter: 34, loss: 5688.834581266114, accuracy: 0.8196\n",
      "iter: 35, loss: 5686.914403962584, accuracy: 0.8196\n",
      "iter: 36, loss: 5683.275736235976, accuracy: 0.8196\n",
      "iter: 37, loss: 5682.053525824047, accuracy: 0.8196\n",
      "iter: 38, loss: 5678.709932478163, accuracy: 0.8196\n",
      "iter: 39, loss: 5675.78143508842, accuracy: 0.8196\n",
      "iter: 40, loss: 5672.461478654886, accuracy: 0.8196\n",
      "iter: 41, loss: 5669.735110643357, accuracy: 0.8196\n",
      "iter: 42, loss: 5666.138385594477, accuracy: 0.8196\n",
      "iter: 43, loss: 5663.728534132755, accuracy: 0.8196\n",
      "iter: 44, loss: 5663.008776300299, accuracy: 0.8196\n",
      "iter: 45, loss: 5660.110843329768, accuracy: 0.8196\n",
      "iter: 46, loss: 5656.239358305957, accuracy: 0.8196\n",
      "iter: 47, loss: 5653.613211516972, accuracy: 0.8196\n",
      "iter: 48, loss: 5650.032033200245, accuracy: 0.8196\n",
      "iter: 49, loss: 5647.193516787045, accuracy: 0.8196\n",
      "iter: 50, loss: 5644.033822773057, accuracy: 0.8196\n",
      "iter: 51, loss: 5641.999477906451, accuracy: 0.8196\n",
      "iter: 52, loss: 5641.496813604648, accuracy: 0.8196\n",
      "iter: 53, loss: 5636.955878912681, accuracy: 0.8196\n",
      "iter: 54, loss: 5631.723581993994, accuracy: 0.8196\n",
      "iter: 55, loss: 5627.055663730291, accuracy: 0.8196\n",
      "iter: 56, loss: 5623.674555554797, accuracy: 0.8196\n",
      "iter: 57, loss: 5620.531028957073, accuracy: 0.8196\n",
      "iter: 58, loss: 5618.575184172505, accuracy: 0.8196\n",
      "iter: 59, loss: 5615.860393222402, accuracy: 0.8196\n",
      "iter: 60, loss: 5612.996472589204, accuracy: 0.8196\n",
      "iter: 61, loss: 5611.101602548387, accuracy: 0.8196\n",
      "iter: 62, loss: 5608.805485024766, accuracy: 0.8196\n",
      "iter: 63, loss: 5604.241350436003, accuracy: 0.8196\n",
      "iter: 64, loss: 5600.94311940476, accuracy: 0.8196\n",
      "iter: 65, loss: 5598.503501506228, accuracy: 0.8196\n",
      "iter: 66, loss: 5595.471415661016, accuracy: 0.8196\n",
      "iter: 67, loss: 5591.374304802482, accuracy: 0.8196\n",
      "iter: 68, loss: 5588.294985055232, accuracy: 0.8196\n",
      "iter: 69, loss: 5585.856498665229, accuracy: 0.8196\n",
      "iter: 70, loss: 5583.141222718861, accuracy: 0.8196\n",
      "iter: 71, loss: 5579.617238755401, accuracy: 0.8196\n",
      "iter: 72, loss: 5575.84261875985, accuracy: 0.8196\n",
      "iter: 73, loss: 5573.868908311307, accuracy: 0.8196\n",
      "iter: 74, loss: 5571.908136734505, accuracy: 0.8196\n",
      "iter: 75, loss: 5569.338853104602, accuracy: 0.8196\n",
      "iter: 76, loss: 5567.252119855559, accuracy: 0.8196\n",
      "iter: 77, loss: 5563.541143935756, accuracy: 0.8196\n",
      "iter: 78, loss: 5561.519185515026, accuracy: 0.8196\n",
      "iter: 79, loss: 5558.131857174508, accuracy: 0.8196\n",
      "iter: 80, loss: 5555.804000157721, accuracy: 0.8196\n",
      "iter: 81, loss: 5552.839096699474, accuracy: 0.8196\n",
      "iter: 82, loss: 5550.283612927068, accuracy: 0.8196\n",
      "iter: 83, loss: 5547.905888285559, accuracy: 0.8196\n",
      "iter: 84, loss: 5545.6040234049615, accuracy: 0.8204\n",
      "iter: 85, loss: 5542.4980562541095, accuracy: 0.819\n",
      "iter: 86, loss: 5540.447841223103, accuracy: 0.819\n",
      "iter: 87, loss: 5536.617155645157, accuracy: 0.819\n",
      "iter: 88, loss: 5534.617430337637, accuracy: 0.819\n",
      "iter: 89, loss: 5532.213579701629, accuracy: 0.819\n",
      "iter: 90, loss: 5527.945262239353, accuracy: 0.8196\n",
      "iter: 91, loss: 5525.488415599581, accuracy: 0.8196\n",
      "iter: 92, loss: 5522.880864791406, accuracy: 0.8196\n",
      "iter: 93, loss: 5519.4225085403805, accuracy: 0.8196\n",
      "iter: 94, loss: 5518.171750007017, accuracy: 0.8196\n",
      "iter: 95, loss: 5516.737721887617, accuracy: 0.8196\n",
      "iter: 96, loss: 5513.704106807242, accuracy: 0.8196\n",
      "iter: 97, loss: 5510.7207186575215, accuracy: 0.8196\n",
      "iter: 98, loss: 5509.57164267141, accuracy: 0.8196\n",
      "iter: 99, loss: 5506.216731155771, accuracy: 0.8196\n",
      "iter: 100, loss: 5504.153553668648, accuracy: 0.8196\n"
     ]
    }
   ],
   "source": [
    "def logistic(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def log_prob(z, y_i):\n",
    "    '''\n",
    "    Returns the log_prob for one point\n",
    "    '''\n",
    "    fz = logistic(z)\n",
    "    return y_i * np.log(fz) + (1 - y_i) * np.log(1 - fz)\n",
    "\n",
    "\n",
    "def neg_log_likelihood(X, w, y):\n",
    "    '''Compute the negative log likelihood'''\n",
    "    L = 0\n",
    "    for _x,_y in zip(X, y):\n",
    "        z = w.dot(_x)\n",
    "        L += log_prob(z=z, y_i=_y)\n",
    "    return -1 * L\n",
    "\n",
    "\n",
    "def fast_logistic(X, w):\n",
    "    '''Compute the logistic function over many data points'''\n",
    "    return 1/(1 + np.exp(-1 * X.dot(w)))\n",
    "\n",
    "\n",
    "def grad(_X, w, _y, lambda_=.5):\n",
    "    '''\n",
    "    Return the gradient\n",
    "    \n",
    "    - https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "    '''\n",
    "    grad = np.zeros_like(w)\n",
    "    \n",
    "    N,D= _X.shape\n",
    "    \n",
    "    b = _X * (fast_logistic(_X, w) - _y).reshape((N, 1))\n",
    "\n",
    "    return np.sum(b, axis=0) + (lambda_ * 2 * w)\n",
    "\n",
    "\n",
    "def squared_l2_norm(w):\n",
    "    '''\n",
    "    $\\sqrt{\\Sigma x_i^2} ^ 2\n",
    "    \n",
    "    '''\n",
    "    # implement this one\n",
    "    return sum(w ** 2)\n",
    "\n",
    "\n",
    "def grad_decent(_X, _y, eta = .0001, lambda_ = 0, tolerance=1e-4, verbose=True, batch_size=None, iters=None):\n",
    "    '''\n",
    "    - Perform gradient ascent\n",
    "    - This function is basically the same as in the Adeline notebook\n",
    "    - Of course, the gradient is different, because it is a different function\n",
    "    '''\n",
    "    w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "    last = 0\n",
    "    losses = []\n",
    "    for i in range(1000):\n",
    "        if i > iters and iters is not None:\n",
    "            break\n",
    "        this_ll = neg_log_likelihood(_X, w, _y)\n",
    "        loss = this_ll + lambda_ * squared_l2_norm(w)\n",
    "        losses.append(loss)\n",
    "        if verbose:\n",
    "            print(\"iter: {}, loss: {}, accuracy: {}\".format(i, loss, accuracy(_X, w, _y)))\n",
    "        if(abs(this_ll - last) < tolerance): break\n",
    "        last = this_ll\n",
    "        \n",
    "        if batch_size is None:\n",
    "            w -= eta * grad(_X, w, _y, lambda_=lambda_)\n",
    "        else:\n",
    "            _N,F = _X.shape\n",
    "            idx = np.random.randint(_N, size=batch_size)\n",
    "            w -= eta * grad(_X[idx], w, _y[idx], lambda_=lambda_)/batch_size\n",
    "        \n",
    "    return w, losses\n",
    "\n",
    "def prediction(X, w, threshold=.5):\n",
    "    '''\n",
    "    - Return a Boolean array of length N.\n",
    "    - The array should be True if the weights dotted with the features for a given instance is greater than .5\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "    return X.dot(w) > threshold\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    '''\n",
    "    Return a value between 0 and 1, showing the fraction of data points which have been classified correctly\n",
    "    '''\n",
    "    return np.mean(prediction(X, w) == y)\n",
    "\n",
    "def init_data(N, dim_):\n",
    "    '''\n",
    "    Initialize data. Note how we generate y below. We know how the data is generated.\n",
    "    '''\n",
    "    w = np.random.uniform(low=-1, high=1, size=dim_)\n",
    "    X = (np.random.rand(dim_ * N) > .5).astype(int)\n",
    "    X = X.reshape(N, dim_)\n",
    "\n",
    "    z_ = X.dot(w) + np.random.uniform(low=-1, high=1, size=X.dot(w).size)\n",
    "\n",
    "    y =  1/(1 + np.exp(-1 * z_)) > .5\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 10000\n",
    "dim_ = 10\n",
    "\n",
    "w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "\n",
    "X, y = init_data(N, dim_)\n",
    "\n",
    "split = int(N/2)\n",
    "\n",
    "X_train = X[0:split]\n",
    "X_test = X[split:]\n",
    "y_train = y[0:split]\n",
    "y_test = y[split:]\n",
    "\n",
    "\n",
    "lambda_ = .1\n",
    "w, losses = grad_decent(X_train, y_train, eta=0.01, tolerance=.0001, iters=100, verbose=True, lambda_=lambda_, batch_size= 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: normalization\n",
    "- Complete the L2 norm function\n",
    "- What does the variable `lambda` do in the code above? \n",
    " - It penalizes for large weights to prevent overfitting\n",
    "- What happens if you set `lambda` to a huge number? What happens if you set `lambda` to a small number?  What should you see in terms of accuracy and the norm of the weights? Try systematically varying lambda\n",
    " - The size of lambda determines how large the penalty will be for large weights. The smaller lambda is, the smaller the penalty for the weights.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "- Computing the gradient can be inefficient. Why? \n",
    " - It can be costly with large n beacuse gradient descent factors in all instances when updating weights\n",
    "- What if instead of computing the whole gradient, we took a sample of the gradient\n",
    "- This is the idea of \"stochastic\" gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: Stochastic gradient descent\n",
    "- How noisy is the optimization if you vary the loss\n",
    "- Print the loss and vary the batch size\n",
    "- How do you think that varying eta will vary the noise \n",
    " - The noise will not vary much because our stepsize (eta) is small\n",
    "- How do you think that varying batch size will vary noise\n",
    " - a larger batch size should result in a smaller loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
