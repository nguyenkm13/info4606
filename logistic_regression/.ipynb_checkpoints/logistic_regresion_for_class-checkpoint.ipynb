{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CU Boulder 4604/5604\n",
    "\n",
    "#### September 28, 2020\n",
    "\n",
    "####  Logistic regression revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression revisited\n",
    "\n",
    "- Last week we learned about logistic regression and regularization\n",
    "- Today, we are going modify the notebook from last week in a few ways \n",
    "\n",
    "##### New stuff:\n",
    "1. I filled in the missing functions from last week if you are curious. Everyone who submitted last week got full credit. These notebooks are a way to get your hands dirty with ML code so you learn the materials more deeply. They are not a way for me to quiz you on what you know. That is what HW, quizzes, exams are for.\n",
    "2. We are going to add a regularization term to the loss function\n",
    "3. We will introduce stochastic gradient descent. The well-known ML researcher [Dave Blei](http://www.cs.columbia.edu/~blei/) says stoachstic gradient descent is like walking from New York to Los Angeles by asking one person at a time for directions. And every person you ask for directions is drunk. Even though this seems like a bad way to get across the country, we will see that is has clear advantages\n",
    "4. I changed the code from gradient ascent to gradient descent (flipping signs as needed). Recall that maximizing log likelihood is the same as minimizing negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annoucements \n",
    "\n",
    "- HW2 is out\n",
    "- This notebook is pretty similar to HW2\n",
    "- In the real world, it is OK to lean on sklearn's implementation of logistic regression\n",
    "- But taking this class gives you a chance to actually understand how it works. This notebook should make the sklearn code much less mysterious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 5793.811507862259, accuracy: 0.8196\n",
      "iter: 1, loss: 4407.599465182677, accuracy: 0.8202\n",
      "iter: 2, loss: 3448.0296205537556, accuracy: 0.8142\n",
      "iter: 3, loss: 2922.776006219685, accuracy: 0.8116\n",
      "iter: 4, loss: 2619.4577087360976, accuracy: 0.8216\n",
      "iter: 5, loss: 2387.9947710155466, accuracy: 0.8276\n",
      "iter: 6, loss: 2191.23446893295, accuracy: 0.8354\n",
      "iter: 7, loss: 2022.8225764784227, accuracy: 0.8456\n",
      "iter: 8, loss: 1879.8570570173936, accuracy: 0.8526\n",
      "iter: 9, loss: 1759.462721965446, accuracy: 0.8566\n",
      "iter: 10, loss: 1658.6821873673289, accuracy: 0.8608\n",
      "iter: 11, loss: 1574.665087755113, accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "def logistic(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def log_prob(z, y_i):\n",
    "    '''\n",
    "    Returns the log_prob for one point\n",
    "    '''\n",
    "    fz = logistic(z)\n",
    "    return y_i * np.log(fz) + (1 - y_i) * np.log(1 - fz)\n",
    "\n",
    "\n",
    "def neg_log_likelihood(X, w, y):\n",
    "    '''Compute the negative log likelihood'''\n",
    "    L = 0\n",
    "    for _x,_y in zip(X, y):\n",
    "        z = w.dot(_x)\n",
    "        L += log_prob(z=z, y_i=_y)\n",
    "    return -1 * L\n",
    "\n",
    "\n",
    "def fast_logistic(X, w):\n",
    "    '''Compute the logistic function over many data points'''\n",
    "    return 1/(1 + np.exp(-1 * X.dot(w)))\n",
    "\n",
    "\n",
    "def grad(_X, w, _y, lambda_=.5):\n",
    "    '''\n",
    "    Return the gradient\n",
    "    \n",
    "    - https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "    '''\n",
    "    grad = np.zeros_like(w)\n",
    "    \n",
    "    N,D= _X.shape\n",
    "    \n",
    "    b = _X * (fast_logistic(_X, w) - _y).reshape((N, 1))\n",
    "\n",
    "    return np.sum(b, axis=0) + (lambda_ * 2 * w)\n",
    "\n",
    "\n",
    "def squared_l2_norm(w):\n",
    "    '''\n",
    "    $\\sqrt{\\Sigma x_i^2} ^ 2\n",
    "    \n",
    "    '''\n",
    "    # implement this one\n",
    "    return sum(w ** 2)\n",
    "\n",
    "\n",
    "def grad_decent(_X, _y, eta = .0001, lambda_ = 0, tolerance=1e-4, verbose=True, batch_size=None, iters=None):\n",
    "    '''\n",
    "    - Perform gradient ascent\n",
    "    - This function is basically the same as in the Adeline notebook\n",
    "    - Of course, the gradient is different, because it is a different function\n",
    "    '''\n",
    "    w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "    last = 0\n",
    "    losses = []\n",
    "    for i in range(1000):\n",
    "        if i > iters and iters is not None:\n",
    "            break\n",
    "        this_ll = neg_log_likelihood(_X, w, _y)\n",
    "        loss = this_ll + lambda_ * squared_l2_norm(w)\n",
    "        losses.append(loss)\n",
    "        if verbose:\n",
    "            print(\"iter: {}, loss: {}, accuracy: {}\".format(i, loss, accuracy(_X, w, _y)))\n",
    "        if(abs(this_ll - last) < tolerance): break\n",
    "        last = this_ll\n",
    "        \n",
    "        if batch_size is None:\n",
    "            w -= eta * grad(_X, w, _y, lambda_=lambda_)\n",
    "        else:\n",
    "            _N,F = _X.shape\n",
    "            idx = np.random.randint(_N, size=batch_size)\n",
    "            w -= eta * grad(_X[idx], w, _y[idx], lambda_=lambda_)/batch_size\n",
    "        \n",
    "    return w, losses\n",
    "\n",
    "def prediction(X, w, threshold=.5):\n",
    "    '''\n",
    "    - Return a Boolean array of length N.\n",
    "    - The array should be True if the weights dotted with the features for a given instance is greater than .5\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "    return X.dot(w) > threshold\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    '''\n",
    "    Return a value between 0 and 1, showing the fraction of data points which have been classified correctly\n",
    "    '''\n",
    "    return np.mean(prediction(X, w) == y)\n",
    "\n",
    "def init_data(N, dim_):\n",
    "    '''\n",
    "    Initialize data. Note how we generate y below. We know how the data is generated.\n",
    "    '''\n",
    "    w = np.random.uniform(low=-1, high=1, size=dim_)\n",
    "    X = (np.random.rand(dim_ * N) > .5).astype(int)\n",
    "    X = X.reshape(N, dim_)\n",
    "\n",
    "    z_ = X.dot(w) + np.random.uniform(low=-1, high=1, size=X.dot(w).size)\n",
    "\n",
    "    y =  1/(1 + np.exp(-1 * z_)) > .5\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 10000\n",
    "dim_ = 10\n",
    "\n",
    "w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "\n",
    "X, y = init_data(N, dim_)\n",
    "\n",
    "split = int(N/2)\n",
    "\n",
    "X_train = X[0:split]\n",
    "X_test = X[split:]\n",
    "y_train = y[0:split]\n",
    "y_test = y[split:]\n",
    "\n",
    "\n",
    "lambda_ = .1\n",
    "w, losses = grad_decent(X_train, y_train, eta=.001, tolerance=.0001, iters=100, verbose=True, lambda_=lambda_, batch_size= None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: normalization\n",
    "- Complete the L2 norm function\n",
    "- What does the variable `lambda` do in the code above? \n",
    " - It penalizes for large weights to prevent overfitting\n",
    "- What happens if you set `lambda` to a huge number? What happens if you set `lambda` to a small number?  What should you see in terms of accuracy and the norm of the weights? Try systematically varying lambda\n",
    " - The size of lambda determines how large the penalty will be for large weights. The smaller lambda is, the smaller the penalty for the weights.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "- Computing the gradient can be inefficient. Why? \n",
    "- What if instead of computing the whole gradient, we took a sample of the gradient\n",
    "- This is the idea of \"stochastic\" gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: Stochastic gradient descent\n",
    "- How noisy is the optimization if you vary the loss\n",
    "- Print the loss and vary the batch size\n",
    "- How do you think that varying eta will vary the noise \n",
    "- How do you think that varying batch size will vary noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
